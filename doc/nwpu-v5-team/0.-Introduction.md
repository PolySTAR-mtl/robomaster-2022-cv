# Introduction

## Translated from https://github.com/nwpu-v5-team/ICRA-RoboMaster-2020-Perception/wiki

During the preparation for the 2020 ICRA RoboMaster Competition:

For the object detection:
- we used SSD framework for object detection
- we used **Pafpn** structure to resolve the bad performance issue of SSD on 
small targets
- we modified **mobilenet-v3** backbone network structure to accelerate the 
computational and training speed.
- we obtained an effective 100-fps computational speed on Jetson Nano for 
object detection
​ ​​ ​ ​​
<div align="center">
​ ​​ <img src = "https://cdn.nlark.com/yuque/0/2020/gif/2394508/1598498514383-1efea4b4-c186-4279-8ae4-2ff53c24d8a1.gif" 
width="50%" height="50%">
  <br> Fig 0.1 Test results with physical robots </br>
</div>
<div align="center">
   <img src = "https://cdn.nlark.com/yuque/0/2020/gif/2394508/1598499056738-e8079a63-26a0-4bd1-a41c-93ae3889e180.gif" width="50%" height="50%">
  <br> Fig 0.2 Test results with physical robots </br>
</div>

For the outpost cameras:

- we used offline 3D reconstruction to deduce the camera poses 
  
<div align="center">
​ ​​ <img src="https://cdn.nlark.com/yuque/0/2020/png/2394508/1598459411670-faa23958-e445-4dd2-a3e6-001de5b3ac4f.png" width="50%" height="50%">
  <br> Fig 0.3 Outpost camera </br>
</div>

- by the means of back projection with camera calibration, the intersection 
point of the back projection ray and the field plane can be obtained, and the
robots can thus be located on the field. In the simulation environment, the 
accuracy within the half-field is less than 20 cm. 

<div align="center">
​ ​​ <img src="https://cdn.nlark.com/yuque/0/2020/gif/2394508/1598498396035-02912ebd-8d48-4d7d-b2bc-30c2c583f053.gif" width="50%" height="50%">
  <br> Fig 0.4 Localization of 4 robots on the map with the outpost camera</br>
</div>
