# Object detection

Object detection includes robot detection and armor plate detection, and 
requires accuracy and real-time frame rate. Our algorithm is based on the 
**anchor-based** SSD framework (Single Shot MultiBox Detector), which is 
suitable for light weight backbone networks. While the detection of small 
targets is not perfect, after adjusting the structure, the overall performance 
has been amazingly improved. The backbone we adopted is based on 
**mobilenet-v3**, and some improvements have been made to adapt it to 512x512 
images. Then the **PaFpn** structure was added to fuse the output multi-scale 
feature layer, so that the small-scale feature layer can have richer semantic 
information, and the large-scale feature has richer feature information, thus 
solving the problem of poor detection of small targets in the SSD framework.

<div align="center">
  <img src = "https://cdn.nlark.com/yuque/0/2020/png/2394508/1598426264776-f23607cd-afe1-4dc2-9fc1-64765818519a.png" width="50%" height="50%">
  <br> Fig 4.1 Object detection network structure </br>
</div>

## Backbone

We **deleted some blocks in the original paper, so that the small features**
**won't be lost during computation while boosting the computational speed**.

<div align="center">
  <img src = "https://cdn.nlark.com/yuque/0/2020/png/2394508/1598413623857-dbd39766-a689-4d27-8b83-f5e1b3283f75.png" width="50%" height="50%">
  <br> Fig 4.2 mobilenet-V3 improved version (orange color indicating the feature output) </br>
</div>

### Network structure

| Structure layer | ker | in | hidden | out | stride | acti | res |
| --- | --- | --- | --- | --- | --- | --- | --- |
| 1（conv） | 3 | 3 | / | 16 | 2 | / | / |
| 2（block） | 3 | 16 | 64 | 24 | 2 | Relu | F |
| 3（block） | 5 | 24 | 72 | 40 | 2 | Relu | F |
| 4（block） | 3 | 40 | 240 | 80 | 2 | hswish | F |
| 5（block） | 3 | 80 | 200 | 80 | 1 | hswish | T |
| 6（block） | 3 | 80 | 480 | 112 | 1 | hswish | F |
| 7（block） | 3 | 112 | 672 | 160 | 1 | hswish | F |
| 8（block） | 5 | 160 | 672 | 160 | 2 | hsiwsh | T |
| 9  (conv) | 1 | 160 | / | 960 | 1 | / | / |
| extra--layer |  |  |  |  |  |  |  |
| 10（block） | 3 | 960 | 256 | 512 | 2 | / | / |
| 11（block） | 3 | 512 | 128 | 256 | 2 | / | / |
| 12（block） | 3 | 256 | 128 | 256 | 2 | / | / |
| 13（block） | 3 | 256 | 64 | 128 | 2 | / | / |

## Necks

During the experiments, we found that the network without **Fpn** structure is 
less effective for small target recognition. The **Fpn** structure can 
effectively improve the semantic information of shallow features, and it can 
also enrich the feature information of deep features, making the localization 
more accurate. 
Compared with not using **Fpn**, the small target recognition effect is 
significantly improved. After algorithm comparison, the **paFpn** structure 
seems to be better. For more information about the **Fpn** structure, please 
refer to the relevant papers. [PaFpn paper url](https://arxiv.org/abs/1803.01534)

<div align="center">
  <img src="https://cdn.nlark.com/yuque/0/2020/png/2376206/1598493766965-46e90ed2-6362-4fbd-b611-316564ca0f19.png?x-oss-process=image%2Fresize%2Cw_1000" width="50%" height="50%">
  <br> Fig 4.3 paFpn network structure </br>
</div>

### Network structure
| Layer | input | hidden（down） | output（up） |
| --- | --- | --- | --- |
| 1 | 672 | 128 | 128 |
| 2 | 960 | 128 | 128 |
| 3 | 512 | 128 | 128 |
| 4 | 256 | 128 | 128 |
| 5 | 256 | 128 | 128 |
| 6 | 128 | 128 | 128 |

## BoxHead

Our BoxHead structure extracts features for regression, and outputs the 
classification and bounding boxes results.
<div align="center">
  <img src="https://cdn.nlark.com/yuque/0/2020/png/2376206/1598493795519-c3de0bf8-a8ad-4a6e-8a6c-97809caec3e2.png?x-oss-process=image%2Fresize%2Cw_1000" width="50%" height="50%">
  <br> Fig 4.4  BoxHead structure </br>
</div>

### Anchors

4774 anchors in total:

| layer | priorbox  size |
| --- | --- |
| 1 | [16,16]   [21,21]  |
| 2 | [40, 40]  [76,76] [13,13] [120,120] [80,80] [20,20] |
| 3 | [148, 148] [186,186] [74,74] [105,105] [296,296] [207,207] |
| 4 | [236,236] [276,276] [118,118] [168,168] [330,330] [472,472] |
| 5 | [324,324][365,365] |
| 6 | [412,412][459,459] |

## Data processing

The training data of the neural network comes from the open-source dataset of 
DJI RoboMaster and some data recorded on our own field. After the backbone 
is trained with the official data set, the BoxHead operation was fine-tuned for 
the AI challenge. Since the official dataset has a higher resolution, training 
directly with these images requires more computational resource, which is not 
conducive to speeding up the process. The official dataset is thus cut into 
512x512 samples for training. In order to achieve the multi-scale training 
effect, 300x300 and 100x100 images were added to the training set. 

Dataset examples:

<div align="center">
  <img src="https://cdn.nlark.com/yuque/0/2020/png/2394508/1598060322327-595e4e34-e233-4813-9099-a7d467c68db8.png?x-oss-process=image%2Fresize%2Cw_1368" width="50%" height="50%">
  <br> Fig 4.5 DJI Robomaster open-source dataset image </br>
</div>

Input data:
<div align="center">
  <img src="https://cdn.nlark.com/yuque/0/2020/png/2394508/1598062575337-8c3c8e94-377d-4dea-82f4-f45562d1ec3c.png" width="10%" height="10%">

  <img src="https://cdn.nlark.com/yuque/0/2020/png/2394508/1598062623968-04dd93bb-8426-40ae-ac6a-26e45c2e4231.png?x-oss-process=image%2Fresize%2Cw_298" width="10%" height="10%">

  <img src="https://cdn.nlark.com/yuque/0/2020/png/2394508/1598062649054-6b224684-7371-4c81-af6e-4261e436a1f4.png" width="10%" height="10%">

  <img src="https://cdn.nlark.com/yuque/0/2020/png/2394508/1598062672215-1b0e7a00-e16d-4b16-8df6-01861dd766a8.png" width="10%" height="10%">

  <img src="https://cdn.nlark.com/yuque/0/2020/png/2394508/1598062715714-4d67bff5-f69f-4a59-ad3f-ac1ffe49cc2f.png" width="10%" height="10%">

  <img src="https://cdn.nlark.com/yuque/0/2020/png/2394508/1598062769965-2a5ce7f1-f984-4c9b-9011-616e58d4dff1.png?x-oss-process=image%2Fresize%2Cw_106" width="10%" height="10%">
  <br> Fig 4.6 Example images used for training </br>
</div>

## Algorithm performance

### Algorithm comparison

|  | mAp | Ap(car) | mAp (finetune) | Ap(car) (finetune) |
| --- | --- | --- | --- | --- |
| SSD-mobileNetv2 | 0.37 | 0.63 | 0.79 | 0.82 |
| SSD-mobileNetV2-pafpn | 0.46 | 0.74 | 0.88 | 0.90 |
| SSD-mobileNetV3 | 0.45 | 0.77 | 0.87 | 0.86 |
| SSD-mobileNetV3-pafpn | 0.55 | 0.81 | 0.90 | 0.90 |

### Real effect on the field

<div align="center">
  <img src = "https://cdn.nlark.com/yuque/0/2020/gif/2394508/1598498514383-1efea4b4-c186-4279-8ae4-2ff53c24d8a1.gif"/>
  <br> Fig 4.7 Real effect on the field </br>
</div>

### Computational time

We conduct the following process conversion:
pytorch ==> onnx ==> tensorrt

After the conversion is completed, we ran on the device to test the runtime of 
a single image (without IO time):

| device | fp32 | fp16 |
| --- | --- | --- |
| jetson  nano | 12-11ms | 6-7ms |
| jetson  tx2 | / | / |
| jetson xaiver | / | / |
| rtx2060 | 3-4ms | 1-2ms |

## Future work (3D-detection)

General idea: from the 4 points of the regression bounding box to the 8 points 
of the regression bounding box, calculate the **epnp** together from the known 
robot size to obtain the 3D position and posture.
